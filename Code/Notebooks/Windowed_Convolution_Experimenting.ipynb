{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eboss Neural Network on Flux Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains my attempts at training a model on the residual flux data instead of the base flux data. It seems to be doing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stone\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn import warnings\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D, Conv2D, Conv1D, InputLayer, Dropout, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import Constraint\n",
    "from keras.initializers import Initializer, glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from Modeling import CrossValidationModeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedInitalizer(Initializer):\n",
    "    \"\"\"\n",
    "    Initalizer used for the Windowed Convolution.\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, window_vect):\n",
    "        # Saving the window vect.\n",
    "        self.window_vect = window_vect\n",
    "    \n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # Generate a row for the kernel.\n",
    "        window_shape = (1, shape[1], shape[2])\n",
    "    \n",
    "        # Generate the appropriate kernel based on the window vect.\n",
    "        weight_list = []\n",
    "        for val in self.window_vect:\n",
    "            weight_list.append(glorot_uniform()(window_shape, dtype) if val == 1 else K.constant(0, shape=window_shape))\n",
    "            \n",
    "        # Join all the rows together and make sure its the correct shape.\n",
    "        weights = K.concatenate(weight_list, 0)\n",
    "        if weights.shape != shape:\n",
    "            raise ValueError('Wrong shape for the kernel.')\n",
    "            \n",
    "        return weights\n",
    "\n",
    "class WindowedConstraint(Constraint):\n",
    "    \"\"\"\n",
    "    Constraint to hold window aspect of the Windowed Convolution\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, window_vect):\n",
    "        # Generate the constraint matrix to use.\n",
    "        self.window_vect = window_vect\n",
    "        self.constraint_matrix = tf.cast(tf.diag(self.window_vect), 'float32')\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        # Apply the constraint matrix.\n",
    "        w = K.dot(K.transpose(w), self.constraint_matrix)\n",
    "        w = K.transpose(w)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_vect = [1,1,1,1,1,1,1,1,1,1, \n",
    "               0,0,0,0,0,0,0,0,0,0,\n",
    "               1,1,1,1,1,1,1,1,1,1]\n",
    "len(window_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_window_vect = [1,1,1,1,1, \n",
    "                      0,0,0,0,0,\n",
    "                      1,1,1,1,1]\n",
    "len(second_window_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(None,4639, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=30, kernel_initializer=WindowedInitalizer(window_vect),\n",
    "                     kernel_constraint=WindowedConstraint(window_vect), \n",
    "                     activation= 'relu'))\n",
    "    model.add(Conv1D(filters=24, kernel_size=15, kernel_initializer=WindowedInitalizer(second_window_vect), \n",
    "                     kernel_constraint=WindowedConstraint(second_window_vect), activation= 'relu', use_bias=False))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=.0001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_conv_flux_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(None,4639, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=2,activation= 'relu'))\n",
    "    model.add(Conv1D(filters=24, kernel_size=2,activation= 'relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def func_model():\n",
    "    main_input = Input(batch_shape=(None,4639, 1))\n",
    "    \n",
    "    # Windowed Side:\n",
    "    c1 = Conv1D(filters=12, kernel_size=30, kernel_initializer=WindowedInitalizer(window_vect), kernel_constraint=WindowedConstraint(window_vect), \n",
    "                     activation= 'elu')(main_input)\n",
    "    c2 = Conv1D(filters=6, kernel_size=15, kernel_initializer=WindowedInitalizer(second_window_vect), kernel_constraint=WindowedConstraint(second_window_vect), \n",
    "                     activation= 'elu')(c1)\n",
    "    #mp1 = MaxPooling1D()(c1)\n",
    "    windowed_flat = Flatten()(c2)\n",
    "    \n",
    "    # Regular convolution\n",
    "    c2_1 = Conv1D(filters=12, kernel_size=3,activation= 'relu')(main_input)\n",
    "    c2_2 = Conv1D(filters=6, kernel_size=3,activation= 'relu')(c2_1)\n",
    "    mp2 = MaxPooling1D()(c2_2)\n",
    "    reg_con_flat = Flatten()(mp2)\n",
    "    \n",
    "    full = Concatenate()([windowed_flat, reg_con_flat])\n",
    "    \n",
    "    d1 = Dense(512,activation='relu')(full)\n",
    "    do1 = Dropout(.5)(d1)\n",
    "    #d2 = Dense(20, activation='relu')(do1)\n",
    "    #do2 = Dropout(.5)(d2)\n",
    "    \n",
    "    main_output = Dense(1, activation='sigmoid')(do1)\n",
    "    \n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=.0001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_conv_flux_model_exp():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(None, 1, 4639, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(1,2), activation='relu', data_format='channels_last'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D((1,2)))\n",
    "    model.add(Conv2D(filters=24, kernel_size=(1,2), activation='relu', data_format='channels_last'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D((1,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_complex():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(None, 4639, 1)))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=10))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=10))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(filters=12, kernel_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=6, kernel_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.50))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.50))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EBOSS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eboss = h5py.File('../../../AstroProj/Data/eboss_flux_full+.hdf5')\n",
    "X, Y = np.array(eboss['flux_values']), np.array(eboss['flux_labels'])\n",
    "res_flux = np.array(eboss['res_flux_values'])\n",
    "ivar = np.array(eboss['ivar_flux_values'])\n",
    "\n",
    "res_flux = np.expand_dims(res_flux, -1)\n",
    "ivar = np.expand_dims(ivar, -1)\n",
    "X = np.expand_dims(X, -1)\n",
    "eboss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(res_flux, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((281, 4639, 1), (281, 4639, 1), (281, 4639, 1), (281, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_flux.shape, ivar.shape, X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 71 samples\n",
      "Epoch 1/12\n",
      " - 7s - loss: 1.0988 - acc: 0.7381 - val_loss: 0.1901 - val_acc: 0.8873\n",
      "Epoch 2/12\n",
      " - 6s - loss: 0.4707 - acc: 0.7190 - val_loss: 0.2058 - val_acc: 0.8310\n",
      "Epoch 3/12\n",
      " - 6s - loss: 0.3598 - acc: 0.7143 - val_loss: 0.2048 - val_acc: 0.8310\n",
      "Epoch 4/12\n",
      " - 6s - loss: 0.3640 - acc: 0.7571 - val_loss: 0.2048 - val_acc: 0.8310\n",
      "Epoch 5/12\n",
      " - 6s - loss: 0.3572 - acc: 0.7714 - val_loss: 0.2044 - val_acc: 0.8873\n",
      "Epoch 6/12\n",
      " - 6s - loss: 0.3839 - acc: 0.7333 - val_loss: 0.2026 - val_acc: 0.8873\n",
      "Epoch 7/12\n",
      " - 6s - loss: 0.3446 - acc: 0.7619 - val_loss: 0.1951 - val_acc: 0.8873\n",
      "Epoch 8/12\n",
      " - 6s - loss: 0.4397 - acc: 0.7190 - val_loss: 0.1953 - val_acc: 0.8873\n",
      "Epoch 9/12\n",
      " - 6s - loss: 0.5091 - acc: 0.7238 - val_loss: 0.2070 - val_acc: 0.8873\n",
      "Epoch 10/12\n",
      " - 6s - loss: 0.4874 - acc: 0.6714 - val_loss: 0.2068 - val_acc: 0.8169\n",
      "Epoch 11/12\n",
      " - 6s - loss: 0.4594 - acc: 0.7095 - val_loss: 0.2121 - val_acc: 0.8169\n",
      "Epoch 12/12\n",
      " - 6s - loss: 0.4994 - acc: 0.7143 - val_loss: 0.2069 - val_acc: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176484a5eb8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = create_windowed_model()\n",
    "mod.fit(X_train, y_train, epochs=12, batch_size=8, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 71 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6021 - acc: 0.6905 - val_loss: 0.2293 - val_acc: 0.8451\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.3283 - acc: 0.7714 - val_loss: 0.2021 - val_acc: 0.9014\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.2870 - acc: 0.8143 - val_loss: 0.1818 - val_acc: 0.9437\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.3054 - acc: 0.8095 - val_loss: 0.1764 - val_acc: 0.9437\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.2655 - acc: 0.8476 - val_loss: 0.1756 - val_acc: 0.9437\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.2350 - acc: 0.8714 - val_loss: 0.1600 - val_acc: 0.9155\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.1712 - acc: 0.9238 - val_loss: 0.1027 - val_acc: 0.9718\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.1543 - acc: 0.9190 - val_loss: 0.0691 - val_acc: 0.9859\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.1567 - acc: 0.9333 - val_loss: 0.1168 - val_acc: 0.9718\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.1698 - acc: 0.9333 - val_loss: 0.0600 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176484a5e48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = create_conv_flux_model()\n",
    "mod.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 71 samples\n",
      "Epoch 1/12\n",
      " - 12s - loss: 1.1941 - acc: 0.8381 - val_loss: 0.3075 - val_acc: 0.9577\n",
      "Epoch 2/12\n",
      " - 11s - loss: 0.2099 - acc: 0.9667 - val_loss: 0.3577 - val_acc: 0.9437\n",
      "Epoch 3/12\n",
      " - 11s - loss: 0.0545 - acc: 0.9905 - val_loss: 0.4067 - val_acc: 0.9577\n",
      "Epoch 4/12\n",
      " - 10s - loss: 0.0163 - acc: 0.9952 - val_loss: 0.4377 - val_acc: 0.9296\n",
      "Epoch 5/12\n",
      " - 11s - loss: 0.0034 - acc: 1.0000 - val_loss: 0.4113 - val_acc: 0.9437\n",
      "Epoch 6/12\n",
      " - 10s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4905 - val_acc: 0.9577\n",
      "Epoch 7/12\n",
      " - 11s - loss: 1.8605e-04 - acc: 1.0000 - val_loss: 0.5210 - val_acc: 0.9437\n",
      "Epoch 8/12\n",
      " - 11s - loss: 1.1243e-04 - acc: 1.0000 - val_loss: 0.5188 - val_acc: 0.9437\n",
      "Epoch 9/12\n",
      " - 10s - loss: 5.7363e-05 - acc: 1.0000 - val_loss: 0.5164 - val_acc: 0.9437\n",
      "Epoch 10/12\n",
      " - 11s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4414 - val_acc: 0.9437\n",
      "Epoch 11/12\n",
      " - 10812s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4432 - val_acc: 0.9437\n",
      "Epoch 12/12\n",
      " - 18s - loss: 2.1863e-05 - acc: 1.0000 - val_loss: 0.5095 - val_acc: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176517819b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = func_model()\n",
    "mod.fit(X_train, y_train, epochs=12, batch_size=8, validation_data=(X_test, y_test), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
