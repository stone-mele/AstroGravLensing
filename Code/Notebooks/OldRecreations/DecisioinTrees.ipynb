{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTrees\n",
    "This notebook will contain my attempts using DecisionTrees on both of eBoss and ManGa datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eBoss Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the eboss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_eBoss = pd.read_csv(\"../../../Data/Astronomy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking out the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_data = raw_data_eBoss.iloc[:, 1:].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating the X, and Y variables, and doing needed preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainable_data.iloc[:, :-1].copy(deep=True)\n",
    "Y = trainable_data.iloc[:, -1:].copy(deep=True)\n",
    "X = X.apply(pd.to_numeric, args={'errors': 'coerce'})\n",
    "X = X.fillna('0')\n",
    "Y = Y.Hits.map({'bad': 0, 'good': 1})\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(ss.fit_transform(X), columns=X.columns)\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_test_precision (no)': array([0.90455942, 0.79920198, 0.78779868, 0.77067164, 0.84114644,\n",
       "        0.79457241, 0.8299054 , 0.78938316]),\n",
       " 'mean_test_precision (yes)': array([0.92147957, 0.9080669 , 0.91608067, 0.91645221, 0.93145723,\n",
       "        0.92151994, 0.93058923, 0.91519533]),\n",
       " 'mean_test_recall (no)': array([0.72994495, 0.7500441 , 0.77763786, 0.75996289, 0.80250327,\n",
       "        0.77035618, 0.80633878, 0.75042735]),\n",
       " 'mean_test_recall (yes)': array([0.96398555, 0.93840769, 0.93256245, 0.93307821, 0.95106951,\n",
       "        0.93183089, 0.94121581, 0.91939863]),\n",
       " 'mean_test_accuracy': array([0.91459075, 0.886121  , 0.88967972, 0.88967972, 0.91459075,\n",
       "        0.89323843, 0.90747331, 0.88256228]),\n",
       " 'mean_test_auc': array([0.60142349, 0.60142349, 0.60142349, 0.65124555, 0.60142349,\n",
       "        0.65124555, 0.60142349, 0.65124555]),\n",
       " 'mean_test_roc_auc': array([0.84696525, 0.8442259 , 0.85510016, 0.84652055, 0.87678639,\n",
       "        0.85109353, 0.87377729, 0.83491299]),\n",
       " 'mean_test_f1': array([0.94086562, 0.92102434, 0.92285527, 0.9235353 , 0.94028719,\n",
       "        0.92559505, 0.93486863, 0.91449573]),\n",
       " 'params': [{'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 7},\n",
       "  {'max_depth': 8},\n",
       "  {'max_depth': 9},\n",
       "  {'max_depth': 10}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining all the scoring metrics that we need to keep track of.\n",
    "scoring_dict = {\n",
    "    'precision (no)': make_scorer(precision_score, pos_label=0),\n",
    "    'precision (yes)': make_scorer(precision_score, pos_label=1),\n",
    "    'recall (no)': make_scorer(recall_score, pos_label=0),\n",
    "    'recall (yes)': make_scorer(recall_score, pos_label=1),\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'auc': make_scorer(auc),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "}\n",
    "\n",
    "# Setting up the params.\n",
    "params_grid = {'max_depth': np.arange(3,11), }\n",
    "\n",
    "# Running the cross-validation with Decision Tree Classifier with 10 folds.\n",
    "grid_clf = GridSearchCV(DecisionTreeClassifier(),\n",
    "                        cv=KFold(n_splits=10, shuffle=True),\n",
    "                        param_grid=params_grid,\n",
    "                        scoring=scoring_dict,\n",
    "                        refit='accuracy', \n",
    "                        return_train_score=False).fit(X,Y)\n",
    "\n",
    "# Extracting relevant metrics. More detailed metrics are available in the cv_results_ object.\n",
    "relevant_metric = {'mean_test_{}'.format(k): grid_clf.cv_results_['mean_test_{}'.format(k)] for k in scoring_dict.keys()}\n",
    "relevant_metric['params'] = grid_clf.cv_results_['params']\n",
    "relevant_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_precision (no) :  0.8411464375877188\n",
      "mean_test_precision (yes) :  0.9314572338037082\n",
      "mean_test_recall (no) :  0.8025032697630562\n",
      "mean_test_recall (yes) :  0.9510695067650043\n",
      "mean_test_accuracy :  0.9145907473309609\n",
      "mean_test_auc :  0.6014234875444839\n",
      "mean_test_roc_auc :  0.8767863882640302\n",
      "mean_test_f1 :  0.9402871914402844\n",
      "params :  {'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "for k,v  in relevant_metric.items():\n",
    "    print(k, \": \", v[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9145907473309609, {'max_depth': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_score_, grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_Manga = pd.read_csv(\"../../../Data/Astronomy20000_Original.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating the X, and Y variables, and doing needed preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data_Manga.iloc[:, 1:-1].copy(deep=True)\n",
    "Y = raw_data_Manga.iloc[:, [-1]].copy(deep=True)\n",
    "X = X.fillna('0')\n",
    "Y = Y.Hits.map({'bad': 0, 'good': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_test_precision (no)': array([0.83621447, 0.83507406, 0.89611522, 0.89566532, 0.87351041,\n",
       "        0.90203714, 0.89819943, 0.9007457 ]),\n",
       " 'mean_test_precision (yes)': array([0.87029168, 0.92518724, 0.71219441, 0.73967092, 0.81737796,\n",
       "        0.75327824, 0.79081341, 0.75890847]),\n",
       " 'mean_test_recall (no)': array([0.97899611, 0.98879642, 0.90777904, 0.9203057 , 0.955049  ,\n",
       "        0.92353713, 0.94012361, 0.92648146]),\n",
       " 'mean_test_recall (yes)': array([0.42407515, 0.4134694 , 0.68394222, 0.67828809, 0.58316125,\n",
       "        0.69884722, 0.67994655, 0.69357227]),\n",
       " 'mean_test_accuracy': array([0.8403713 , 0.84508465, 0.85181801, 0.85970566, 0.8623509 ,\n",
       "        0.86744902, 0.87514429, 0.86821855]),\n",
       " 'mean_test_auc': array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
       " 'mean_test_roc_auc': array([0.70153563, 0.70113291, 0.79586063, 0.7992969 , 0.76910512,\n",
       "        0.81119218, 0.81003508, 0.81002686]),\n",
       " 'mean_test_f1': array([0.56992943, 0.57111448, 0.69729296, 0.70700758, 0.67826016,\n",
       "        0.72482493, 0.73099945, 0.72429174]),\n",
       " 'params': [{'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 7},\n",
       "  {'max_depth': 8},\n",
       "  {'max_depth': 9},\n",
       "  {'max_depth': 10}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining all the scoring metrics that we need to keep track of.\n",
    "scoring_dict = {\n",
    "    'precision (no)': make_scorer(precision_score, pos_label=0),\n",
    "    'precision (yes)': make_scorer(precision_score, pos_label=1),\n",
    "    'recall (no)': make_scorer(recall_score, pos_label=0),\n",
    "    'recall (yes)': make_scorer(recall_score, pos_label=1),\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'auc': make_scorer(auc, reorder=True),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "}\n",
    "\n",
    "# Setting up the params.\n",
    "params_grid = {'max_depth': np.arange(3,11)}\n",
    "\n",
    "# Running the cross-validation 10 folds with decision tree classifier.\n",
    "grid_clf = GridSearchCV(DecisionTreeClassifier(),\n",
    "                        cv=KFold(n_splits=10, shuffle=True),\n",
    "                        param_grid=params_grid,\n",
    "                        scoring=scoring_dict,\n",
    "                        refit='accuracy', \n",
    "                        return_train_score=False)\n",
    "grid_clf.fit(X,Y)\n",
    "\n",
    "# Extracting relevant metrics. More detailed metrics are available in the cv_results_ object.\n",
    "relevant_metric = {'mean_test_{}'.format(k): grid_clf.cv_results_['mean_test_{}'.format(k)] for k in scoring_dict.keys()}\n",
    "relevant_metric['params'] = grid_clf.cv_results_['params']\n",
    "relevant_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
