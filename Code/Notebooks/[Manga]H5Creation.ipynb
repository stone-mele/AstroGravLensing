{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga H5 Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File used to create H5 dataset for manga dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import Modeling\n",
    "reload(Modeling)\n",
    "import Modeling\n",
    "from Modeling import CrossValidationModeler, EbossWaveSpectraDataset, MangaWaveSpectraDataset\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D, Conv2D, Conv1D, InputLayer, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate Needed values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2184)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manga_data = pd.read_csv('../../Data/Astronomy20000_Original.csv')\n",
    "# Extract the duplicates\n",
    "excat_duplicates = manga_data[manga_data.duplicated(keep='last')]\n",
    "id_duplicates_excluding_excat = manga_data[~manga_data.duplicated(keep=False) & manga_data.duplicated(subset='id', keep=False)]\n",
    "# Drop them.\n",
    "manga_data.drop(excat_duplicates.index, inplace=True)\n",
    "manga_data.drop(id_duplicates_excluding_excat.index, inplace=True)\n",
    "manga_ids = manga_data[['id', 'Hits']]\n",
    "\n",
    "# Place fitler here.\n",
    "manga_ids = manga_ids[manga_ids.id.str.contains('multiline')]\n",
    "\n",
    "# Collectors for files\n",
    "not_found_files_general = []\n",
    "found_files_general = []\n",
    "\n",
    "# Creating variables needed for file existance checking.\n",
    "file_name_template = 'C:/Users/stone/Desktop/Summer2018/AstroProj/Data/RawManga\\HIT_RSS_mpl5_{plate_id}_{mjd}_fiber_{fiber_id}_{type}.fits'\n",
    "file_list = set(glob('C:/Users/stone/Desktop/Summer2018/AstroProj/Data/RawManga/*.fits'))\n",
    "\n",
    "# For each id in the initial dataset parse it and read in the values.\n",
    "for _, e_id, hit in manga_ids.itertuples():\n",
    "    plate_id, mjd, _, fiber_id, file_type = e_id.split('_')\n",
    "    file_to_read = file_name_template.format(plate_id=plate_id, mjd=mjd, fiber_id=fiber_id, type=file_type)\n",
    "    if file_to_read in file_list:\n",
    "        fits.open(file_to_read)\n",
    "        found_files_general.append((e_id, file_to_read, file_type, hit))\n",
    "    else:\n",
    "        not_found_files_general.append(e_id)\n",
    "        \n",
    "manga_locations = pd.DataFrame(found_files_general, columns=['id', 'location', 'file_type', 'hit'])\n",
    "manga_locations.to_csv('manga_locations_multiline.csv')\n",
    "len(not_found_files_general), len(found_files_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the h5py dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file['flux_values']\n",
    "del file['flux_labels']\n",
    "del file['flux_ids']\n",
    "del file['res_flux_values']\n",
    "del file['ivar_flux_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwsd = MangaWaveSpectraDataset('CSVS/manga_locations_oneline.csv')\n",
    "num_of_spectra = len(mwsd)\n",
    "\n",
    "#del file['flux_values']\n",
    "#del file['flux_labels']\n",
    "#del file['flux_ids']\n",
    "\n",
    "#file.close()\n",
    "\n",
    "#H5 File setup\n",
    "file = h5py.File('../../Data/manga_flux_oneline+.hdf5', mode='w')\n",
    "file.create_dataset('flux_values', shape=(num_of_spectra, 4563))\n",
    "file.create_dataset('res_flux_values', shape=(num_of_spectra, 4563))\n",
    "file.create_dataset('ivar_flux_values', shape=(num_of_spectra, 4563))\n",
    "file.create_dataset('ivar_rescaled_flux_values', shape=(num_of_spectra, 4563))\n",
    "file.create_dataset('flux_labels', shape=(num_of_spectra, 1))\n",
    "file.create_dataset('flux_ids', shape=(num_of_spectra, 1), dtype='S33')\n",
    "\n",
    "# Place this in the for loop.\n",
    "label_mapper = lambda x: 0 if x == 'bad' else 1\n",
    "mwsd.spectra_infos.id = mwsd.spectra_infos.id.astype('S')\n",
    "for ix in range(len(mwsd)):\n",
    "    row = mwsd[ix]\n",
    "    file['flux_values'][ix] = row[0]\n",
    "    file['flux_labels'][ix] = label_mapper(row[1].hit)\n",
    "    file['flux_ids'][ix] = row[1].id\n",
    "    \n",
    "    file['ivar_flux_values'][ix] = mwsd.get_ext_data(ix, 4)[0]\n",
    "    file['ivar_rescaled_flux_values'][ix] = mwsd.get_ext_data(ix, 5)[0]\n",
    "    file['res_flux_values'][ix] = mwsd.get_ext_data(ix, 7)[0]\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
